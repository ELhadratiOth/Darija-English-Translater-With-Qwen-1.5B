{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\n\nfrom kaggle_secrets import UserSecretsClient\nimport os\nos.environ['KAGGLE_USERNAME'] = UserSecretsClient().get_secret(\"kaggle_username\")\nos.environ['KAGGLE_KEY'] = UserSecretsClient().get_secret(\"kaggle_key\")\nwandb.login(key= UserSecretsClient().get_secret(\"WANDB_KEY\"))\n\nhf_token =  UserSecretsClient().get_secret(\"HUGGING_FACE\")\n!huggingface-cli login --token {hf_token}","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:35:29.794195Z","iopub.execute_input":"2025-03-01T21:35:29.794540Z","iopub.status.idle":"2025-03-01T21:35:39.583741Z","shell.execute_reply.started":"2025-03-01T21:35:29.794508Z","shell.execute_reply":"2025-03-01T21:35:39.582693Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mothmanelhadrati\u001b[0m (\u001b[33mothmanelhadrati-ensa-al-hoceima\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `gg` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `gg`\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!kaggle kernels output othmandone/darijafinetuning4 -p /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:35:46.953478Z","iopub.execute_input":"2025-03-01T21:35:46.953757Z","iopub.status.idle":"2025-03-01T21:37:00.185537Z","shell.execute_reply.started":"2025-03-01T21:35:46.953736Z","shell.execute_reply":"2025-03-01T21:37:00.184596Z"}},"outputs":[{"name":"stdout","text":"Output file downloaded to /kaggle/working/FIneTune/dataInOut.csv\nOutput file downloaded to /kaggle/working/FIneTune/datasets/train.json\nOutput file downloaded to /kaggle/working/FIneTune/datasets/val.json\nOutput file downloaded to /kaggle/working/FIneTune/models/README.md\nOutput file downloaded to /kaggle/working/FIneTune/models/adapter_config.json\nOutput file downloaded to /kaggle/working/FIneTune/models/adapter_model.safetensors\nOutput file downloaded to /kaggle/working/FIneTune/models/added_tokens.json\nOutput file downloaded to /kaggle/working/FIneTune/models/all_results.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/README.md\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/adapter_config.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/adapter_model.safetensors\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/added_tokens.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/merges.txt\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/optimizer.pt\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/rng_state_0.pth\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/rng_state_1.pth\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/scheduler.pt\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/special_tokens_map.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/tokenizer.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/tokenizer_config.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/trainer_state.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/training_args.bin\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-14000/vocab.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/README.md\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/adapter_config.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/adapter_model.safetensors\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/added_tokens.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/merges.txt\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/optimizer.pt\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/rng_state_0.pth\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/rng_state_1.pth\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/scheduler.pt\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/special_tokens_map.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/tokenizer.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/tokenizer_config.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/trainer_state.json\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/training_args.bin\nOutput file downloaded to /kaggle/working/FIneTune/models/checkpoint-15000/vocab.json\nOutput file downloaded to /kaggle/working/FIneTune/models/eval_results.json\nOutput file downloaded to /kaggle/working/FIneTune/models/merges.txt\nOutput file downloaded to /kaggle/working/FIneTune/models/special_tokens_map.json\nOutput file downloaded to /kaggle/working/FIneTune/models/tokenizer.json\nOutput file downloaded to /kaggle/working/FIneTune/models/tokenizer_config.json\nOutput file downloaded to /kaggle/working/FIneTune/models/train_results.json\nOutput file downloaded to /kaggle/working/FIneTune/models/trainer_log.jsonl\nOutput file downloaded to /kaggle/working/FIneTune/models/trainer_state.json\nOutput file downloaded to /kaggle/working/FIneTune/models/training_args.bin\nOutput file downloaded to /kaggle/working/FIneTune/models/training_loss.png\nOutput file downloaded to /kaggle/working/FIneTune/models/vocab.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.dockerignore\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.env.local\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/HEAD\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/config\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/description\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/applypatch-msg.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/commit-msg.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/fsmonitor-watchman.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/post-update.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/pre-applypatch.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/pre-commit.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/pre-merge-commit.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/pre-push.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/pre-rebase.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/pre-receive.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/prepare-commit-msg.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/push-to-checkout.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/hooks/update.sample\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/index\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/info/exclude\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/logs/HEAD\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/logs/refs/heads/main\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/logs/refs/remotes/origin/HEAD\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/objects/pack/pack-f059357479d9a8b70064e6c2dec4b6c0d488aa9b.idx\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/objects/pack/pack-f059357479d9a8b70064e6c2dec4b6c0d488aa9b.pack\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/packed-refs\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/refs/heads/main\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/refs/remotes/origin/HEAD\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.git/shallow\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.gitattributes\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/CODE_OF_CONDUCT.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/CONTRIBUTING.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/ISSUE_TEMPLATE/1-bug-report.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/ISSUE_TEMPLATE/2-feature-request.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/ISSUE_TEMPLATE/config.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/PULL_REQUEST_TEMPLATE.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/SECURITY.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/workflows/label_issue.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/workflows/publish.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.github/workflows/tests.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.gitignore\nOutput file downloaded to /kaggle/working/LLaMA-Factory/.pre-commit-config.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/CITATION.cff\nOutput file downloaded to /kaggle/working/LLaMA-Factory/LICENSE\nOutput file downloaded to /kaggle/working/LLaMA-Factory/MANIFEST.in\nOutput file downloaded to /kaggle/working/LLaMA-Factory/Makefile\nOutput file downloaded to /kaggle/working/LLaMA-Factory/README.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/README_zh.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/assets/benchmark.svg\nOutput file downloaded to /kaggle/working/LLaMA-Factory/assets/logo.png\nOutput file downloaded to /kaggle/working/LLaMA-Factory/assets/wechat.jpg\nOutput file downloaded to /kaggle/working/LLaMA-Factory/assets/wechat_npu.jpg\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/README.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/README_zh.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/alpaca_en_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/alpaca_zh_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/belle_multiturn/belle_multiturn.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/c4_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/dataset_info.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/dpo_en_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/dpo_zh_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/glaive_toolcall_en_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/glaive_toolcall_zh_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/hh_rlhf_en/hh_rlhf_en.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/identity.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/kto_en_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_audio_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/1.jpg\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/1.mp3\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/1.mp4\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/2.avi\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/2.jpg\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/2.wav\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/3.flac\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/3.jpg\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_demo_data/3.mp4\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/mllm_video_demo.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/ultra_chat/ultra_chat.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/data/wiki_demo.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/docker/docker-cuda/Dockerfile\nOutput file downloaded to /kaggle/working/LLaMA-Factory/docker/docker-cuda/docker-compose.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/docker/docker-npu/Dockerfile\nOutput file downloaded to /kaggle/working/LLaMA-Factory/docker/docker-npu/docker-compose.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/docker/docker-rocm/Dockerfile\nOutput file downloaded to /kaggle/working/LLaMA-Factory/docker/docker-rocm/docker-compose.yml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/ceval/ceval.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/ceval/ceval.zip\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/ceval/mapping.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/cmmlu/cmmlu.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/cmmlu/cmmlu.zip\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/cmmlu/mapping.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/mmlu/mapping.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/mmlu/mmlu.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/evaluation/mmlu/mmlu.zip\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/README.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/README_zh.md\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/accelerate/fsdp_config.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/deepspeed/ds_z0_config.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/deepspeed/ds_z2_config.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/deepspeed/ds_z2_offload_config.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/deepspeed/ds_z3_config.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/deepspeed/ds_z3_offload_config.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/adam_mini/qwen2_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/apollo/llama3_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/badam/llama3_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/fsdp_qlora/llama3_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/fsdp_qlora/train.sh\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/galore/llama3_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/llama_pro/expand.sh\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/llama_pro/llama3_freeze_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/loraplus/llama3_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/mod/llama3_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/nlg_eval/llama3_lora_predict.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/pissa/init.sh\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/extras/pissa/llama3_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/inference/llama3.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/inference/llama3_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/inference/llama3_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/inference/llama3_vllm.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/inference/llava1_5.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/inference/qwen2_vl.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/merge_lora/llama3_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/merge_lora/llama3_gptq.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/merge_lora/llama3_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/merge_lora/qwen2vl_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_full/llama3_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_full/qwen2vl_full_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/darija_finetune.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_dpo.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_eval.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_kto.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_ppo.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_pretrain.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_reward.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_sft_ds3.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_lora_sft_ray.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llama3_preprocess.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/llava1_5_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/qwen2vl_lora_dpo.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_lora/qwen2vl_lora_sft.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_qlora/llama3_lora_sft_aqlm.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_qlora/llama3_lora_sft_awq.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_qlora/llama3_lora_sft_bnb_npu.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_qlora/llama3_lora_sft_gptq.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/examples/train_qlora/llama3_lora_sft_otfq.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/pyproject.toml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/requirements.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/api_example/test_image.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/api_example/test_toolcall.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/convert_ckpt/llamafy_baichuan2.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/convert_ckpt/llamafy_qwen.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/llama_pro.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/loftq_init.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/pissa_init.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/stat_utils/cal_flops.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/stat_utils/cal_lr.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/stat_utils/cal_mfu.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/stat_utils/cal_ppl.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/stat_utils/length_cdf.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/scripts/vllm_infer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/setup.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/api.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory.egg-info/PKG-INFO\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory.egg-info/SOURCES.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory.egg-info/dependency_links.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory.egg-info/entry_points.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory.egg-info/requires.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory.egg-info/top_level.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/__pycache__/cli.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/__pycache__/launcher.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/__pycache__/app.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/__pycache__/chat.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/__pycache__/common.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/__pycache__/protocol.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/app.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/chat.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/common.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/api/protocol.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/__pycache__/base_engine.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/__pycache__/chat_model.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/__pycache__/hf_engine.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/__pycache__/vllm_engine.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/base_engine.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/chat_model.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/hf_engine.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/chat/vllm_engine.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/cli.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/collator.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/converter.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/data_utils.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/formatter.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/loader.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/mm_plugin.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/parser.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/template.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/__pycache__/tool_utils.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/collator.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/converter.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/data_utils.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/formatter.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/loader.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/mm_plugin.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/parser.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__pycache__/feedback.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__pycache__/pairwise.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__pycache__/pretrain.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__pycache__/processor_utils.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__pycache__/supervised.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/__pycache__/unsupervised.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/feedback.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/pairwise.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/pretrain.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/processor_utils.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/supervised.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/processor/unsupervised.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/template.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/data/tool_utils.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/eval/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/eval/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/eval/__pycache__/evaluator.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/eval/__pycache__/template.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/eval/evaluator.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/eval/template.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__pycache__/constants.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__pycache__/env.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__pycache__/logging.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__pycache__/misc.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__pycache__/packages.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/__pycache__/ploting.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/constants.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/env.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/logging.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/misc.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/packages.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/extras/ploting.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/data_args.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/evaluation_args.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/finetuning_args.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/generating_args.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/model_args.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/parser.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/__pycache__/training_args.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/data_args.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/evaluation_args.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/finetuning_args.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/generating_args.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/model_args.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/parser.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/hparams/training_args.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/launcher.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/__pycache__/adapter.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/__pycache__/loader.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/__pycache__/patcher.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/adapter.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/loader.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/attention.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/checkpointing.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/embedding.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/liger_kernel.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/longlora.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/misc.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/mod.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/moe.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/packing.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/quantization.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/rope.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/unsloth.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/valuehead.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/__pycache__/visual.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/attention.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/checkpointing.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/embedding.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/liger_kernel.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/longlora.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/misc.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/mod.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/moe.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/packing.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/quantization.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/rope.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/unsloth.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/valuehead.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/model_utils/visual.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/model/patcher.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/__pycache__/callbacks.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/__pycache__/trainer_utils.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/__pycache__/tuner.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/callbacks.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/dpo/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/dpo/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/dpo/__pycache__/trainer.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/dpo/__pycache__/workflow.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/dpo/trainer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/dpo/workflow.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/kto/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/kto/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/kto/__pycache__/trainer.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/kto/__pycache__/workflow.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/kto/trainer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/kto/workflow.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/__pycache__/ppo_utils.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/__pycache__/trainer.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/__pycache__/workflow.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/ppo_utils.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/trainer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/ppo/workflow.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/pt/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/pt/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/pt/__pycache__/trainer.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/pt/__pycache__/workflow.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/pt/trainer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/pt/workflow.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/__pycache__/metric.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/__pycache__/trainer.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/__pycache__/workflow.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/metric.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/trainer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/rm/workflow.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/__pycache__/metric.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/__pycache__/trainer.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/__pycache__/workflow.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/metric.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/trainer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/test_utils.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/trainer_utils.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/train/tuner.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/chatter.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/common.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/control.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/css.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/engine.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/interface.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/locales.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/manager.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/__pycache__/runner.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/chatter.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/common.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__init__.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/__init__.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/chatbot.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/data.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/eval.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/export.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/infer.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/top.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/__pycache__/train.cpython-310.pyc\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/chatbot.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/data.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/eval.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/export.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/infer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/top.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/components/train.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/control.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/css.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/engine.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/interface.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/locales.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/manager.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/llamafactory/webui/runner.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/train.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/src/webui.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/processor/test_feedback.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/processor/test_pairwise.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/processor/test_processor_utils.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/processor/test_supervised.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/processor/test_unsupervised.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/test_collator.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/test_converter.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/test_formatter.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/test_mm_plugin.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/data/test_template.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/e2e/test_chat.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/e2e/test_train.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/eval/test_eval_template.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/model_utils/test_attention.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/model_utils/test_checkpointing.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/model_utils/test_misc.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/model_utils/test_packing.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/model_utils/test_visual.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/test_base.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/test_freeze.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/test_full.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/test_lora.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/model/test_pissa.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/tests/train/test_sft_trainer.py\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/files/config.yaml\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/files/output.log\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/files/requirements.txt\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/files/wandb-metadata.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/files/wandb-summary.json\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/logs/debug-internal.log\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/logs/debug.log\nOutput file downloaded to /kaggle/working/LLaMA-Factory/wandb/run-20250301_173334-u3mue39u/run-u3mue39u.wandb\nOutput file downloaded to /kaggle/working/darijafinetuning3.log\nKernel log downloaded to /kaggle/working/darijafinetuning4.log \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import shutil\n\nfor i in range(14000, 14001, 1000):\n    try :\n        shutil.rmtree(f\"/kaggle/working/FIneTune/models/checkpoint-{i}\")\n        print(f\"/kaggle/working/FIneTune/models/checkpoint-{i} has been deleted\")\n    except : \n        pass\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:37:00.186927Z","iopub.execute_input":"2025-03-01T21:37:00.187209Z","iopub.status.idle":"2025-03-01T21:37:00.378206Z","shell.execute_reply.started":"2025-03-01T21:37:00.187186Z","shell.execute_reply":"2025-03-01T21:37:00.377309Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/FIneTune/models/checkpoint-14000 has been deleted\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n!cd LLaMA-Factory && pip install -e .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:37:00.379867Z","iopub.execute_input":"2025-03-01T21:37:00.380123Z","iopub.status.idle":"2025-03-01T21:37:35.017095Z","shell.execute_reply.started":"2025-03-01T21:37:00.380103Z","shell.execute_reply":"2025-03-01T21:37:35.015969Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\nObtaining file:///kaggle/working/LLaMA-Factory\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 (from llamafactory==0.9.2.dev0)\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting datasets<=3.2.0,>=2.16.0 (from llamafactory==0.9.2.dev0)\n  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: accelerate<=1.2.1,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.2.1)\nCollecting peft<=0.12.0,>=0.11.1 (from llamafactory==0.9.2.dev0)\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nCollecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.2.dev0)\n  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.21.0)\nCollecting gradio<=5.18.0,>=4.38.0 (from llamafactory==0.9.2.dev0)\n  Downloading gradio-5.18.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.2.3)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.13.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.8.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.2.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.9.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.20.3)\nCollecting uvicorn (from llamafactory==0.9.2.dev0)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (2.11.0a2)\nCollecting fastapi (from llamafactory==0.9.2.dev0)\n  Downloading fastapi-0.115.10-py3-none-any.whl.metadata (27 kB)\nCollecting sse-starlette (from llamafactory==0.9.2.dev0)\n  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (3.7.5)\nCollecting fire (from llamafactory==0.9.2.dev0)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (24.2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (6.0.2)\nRequirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (1.26.4)\nCollecting av (from llamafactory==0.9.2.dev0)\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.2.dev0) (0.10.2.post1)\nCollecting tyro<0.9.0 (from llamafactory==0.9.2.dev0)\n  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (5.9.5)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.70.16)\nCollecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0)\n  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.11.12)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.7.1)\nCollecting ffmpy (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.2 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.28.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.10.12)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (11.0.0)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nCollecting ruff>=0.9.3 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (4.12.2)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.2->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (14.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.4.7)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.2.dev0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.2.dev0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.2.dev0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.2.dev0) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.2.dev0) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0.0->llamafactory==0.9.2.dev0) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.2.dev0) (2025.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.9.2.dev0) (2.29.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->llamafactory==0.9.2.dev0) (2024.11.6)\nRequirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.2.dev0) (13.9.4)\nCollecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.2.dev0)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.2.dev0) (0.14.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.2.dev0) (2.5.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (4.4.2)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.60.0)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.5.0.post1)\nRequirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.2.dev0) (1.1.0)\nCollecting anyio<5.0,>=3.0 (from gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0)\n  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.2.2)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (1.18.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.0.7)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.2.dev0) (0.43.0)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.2.dev0) (4.3.6)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.2.dev0) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory==0.9.2.dev0) (2.3.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (2.19.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.2.dev0) (3.5.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.2.dev0) (1.17.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (3.4.2)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory==0.9.2.dev0) (1.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.18.0,>=4.38.0->llamafactory==0.9.2.dev0) (1.5.4)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.2.dev0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0.0->llamafactory==0.9.2.dev0) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0->llamafactory==0.9.2.dev0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0.0->llamafactory==0.9.2.dev0) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.2.dev0) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0.0->llamafactory==0.9.2.dev0) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.2.dev0) (0.1.2)\nDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio-5.18.0-py3-none-any.whl (62.3 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.10-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\nDownloading anyio-4.8.0-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading starlette-0.46.0-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nBuilding wheels for collected packages: llamafactory, fire\n  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llamafactory: filename=llamafactory-0.9.2.dev0-0.editable-py3-none-any.whl size=25803 sha256=987e6294ccec4296aea81223651270cc505d96db39f851c678550dda5e6675b1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-oqhl7xoi/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=c172f2665ea337f11c6161fbccdd2b8fd624d832a2a5e0b663edfb0208e61e4b\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built llamafactory fire\nInstalling collected packages: uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, markupsafe, fsspec, fire, ffmpy, av, anyio, starlette, tyro, sse-starlette, safehttpx, gradio-client, fastapi, transformers, datasets, trl, peft, gradio, llamafactory\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.12.0\n    Uninstalling fsspec-2024.12.0:\n      Successfully uninstalled fsspec-2024.12.0\n  Attempting uninstall: anyio\n    Found existing installation: anyio 3.7.1\n    Uninstalling anyio-3.7.1:\n      Successfully uninstalled anyio-3.7.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.3.1\n    Uninstalling datasets-3.3.1:\n      Successfully uninstalled datasets-3.3.1\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed anyio-4.8.0 av-14.2.0 datasets-3.2.0 fastapi-0.115.10 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.9.0 gradio-5.18.0 gradio-client-1.7.2 llamafactory-0.9.2.dev0 markupsafe-2.1.5 peft-0.12.0 python-multipart-0.0.20 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.2.1 starlette-0.46.0 tomlkit-0.13.2 transformers-4.49.0 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import json\n\nfile_path = \"/kaggle/working/LLaMA-Factory/data/dataset_info.json\" \n\nnew_content = {    \n    \"identity\": {\n      \"file_name\": \"identity.json\"\n    },\n    \"alpaca_en_demo\": {\n      \"file_name\": \"alpaca_en_demo.json\"\n    },\n    \"alpaca_zh_demo\": {\n      \"file_name\": \"alpaca_zh_demo.json\"\n    },\n    \"glaive_toolcall_en_demo\": {\n      \"file_name\": \"glaive_toolcall_en_demo.json\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"tools\": \"tools\"\n      }\n    },\n    \"glaive_toolcall_zh_demo\": {\n      \"file_name\": \"glaive_toolcall_zh_demo.json\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"tools\": \"tools\"\n      }\n    },\n    \"mllm_demo\": {\n      \"file_name\": \"mllm_demo.json\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"images\": \"images\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"mllm_audio_demo\": {\n      \"file_name\": \"mllm_audio_demo.json\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"audios\": \"audios\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"mllm_video_demo\": {\n      \"file_name\": \"mllm_video_demo.json\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"videos\": \"videos\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"alpaca_en\": {\n      \"hf_hub_url\": \"llamafactory/alpaca_en\",\n      \"ms_hub_url\": \"llamafactory/alpaca_en\",\n      \"om_hub_url\": \"HaM/alpaca_en\"\n    },\n    \"alpaca_zh\": {\n      \"hf_hub_url\": \"llamafactory/alpaca_zh\",\n      \"ms_hub_url\": \"llamafactory/alpaca_zh\"\n    },\n    \"alpaca_gpt4_en\": {\n      \"hf_hub_url\": \"llamafactory/alpaca_gpt4_en\",\n      \"ms_hub_url\": \"llamafactory/alpaca_gpt4_en\"\n    },\n    \"alpaca_gpt4_zh\": {\n      \"hf_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n      \"ms_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n      \"om_hub_url\": \"State_Cloud/alpaca-gpt4-data-zh\"\n    },\n    \"glaive_toolcall_en\": {\n      \"hf_hub_url\": \"llamafactory/glaive_toolcall_en\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"tools\": \"tools\"\n      }\n    },\n    \"glaive_toolcall_zh\": {\n      \"hf_hub_url\": \"llamafactory/glaive_toolcall_zh\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"tools\": \"tools\"\n      }\n    },\n    \"lima\": {\n      \"hf_hub_url\": \"llamafactory/lima\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"guanaco\": {\n      \"hf_hub_url\": \"JosephusCheung/GuanacoDataset\",\n      \"ms_hub_url\": \"AI-ModelScope/GuanacoDataset\"\n    },\n    \"belle_2m\": {\n      \"hf_hub_url\": \"BelleGroup/train_2M_CN\",\n      \"ms_hub_url\": \"AI-ModelScope/train_2M_CN\"\n    },\n    \"belle_1m\": {\n      \"hf_hub_url\": \"BelleGroup/train_1M_CN\",\n      \"ms_hub_url\": \"AI-ModelScope/train_1M_CN\"\n    },\n    \"belle_0.5m\": {\n      \"hf_hub_url\": \"BelleGroup/train_0.5M_CN\",\n      \"ms_hub_url\": \"AI-ModelScope/train_0.5M_CN\"\n    },\n    \"belle_dialog\": {\n      \"hf_hub_url\": \"BelleGroup/generated_chat_0.4M\",\n      \"ms_hub_url\": \"AI-ModelScope/generated_chat_0.4M\"\n    },\n    \"belle_math\": {\n      \"hf_hub_url\": \"BelleGroup/school_math_0.25M\",\n      \"ms_hub_url\": \"AI-ModelScope/school_math_0.25M\"\n    },\n    \"belle_multiturn\": {\n      \"script_url\": \"belle_multiturn\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"ultra_chat\": {\n      \"script_url\": \"ultra_chat\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"open_platypus\": {\n      \"hf_hub_url\": \"garage-bAInd/Open-Platypus\",\n      \"ms_hub_url\": \"AI-ModelScope/Open-Platypus\"\n    },\n    \"codealpaca\": {\n      \"hf_hub_url\": \"sahil2801/CodeAlpaca-20k\",\n      \"ms_hub_url\": \"AI-ModelScope/CodeAlpaca-20k\"\n    },\n    \"alpaca_cot\": {\n      \"hf_hub_url\": \"QingyiSi/Alpaca-CoT\",\n      \"ms_hub_url\": \"AI-ModelScope/Alpaca-CoT\"\n    },\n    \"openorca\": {\n      \"hf_hub_url\": \"Open-Orca/OpenOrca\",\n      \"ms_hub_url\": \"AI-ModelScope/OpenOrca\",\n      \"columns\": {\n        \"prompt\": \"question\",\n        \"response\": \"response\",\n        \"system\": \"system_prompt\"\n      }\n    },\n    \"slimorca\": {\n      \"hf_hub_url\": \"Open-Orca/SlimOrca\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"mathinstruct\": {\n      \"hf_hub_url\": \"TIGER-Lab/MathInstruct\",\n      \"ms_hub_url\": \"AI-ModelScope/MathInstruct\",\n      \"columns\": {\n        \"prompt\": \"instruction\",\n        \"response\": \"output\"\n      }\n    },\n    \"firefly\": {\n      \"hf_hub_url\": \"YeungNLP/firefly-train-1.1M\",\n      \"columns\": {\n        \"prompt\": \"input\",\n        \"response\": \"target\"\n      }\n    },\n    \"wikiqa\": {\n      \"hf_hub_url\": \"wiki_qa\",\n      \"columns\": {\n        \"prompt\": \"question\",\n        \"response\": \"answer\"\n      }\n    },\n    \"webqa\": {\n      \"hf_hub_url\": \"suolyer/webqa\",\n      \"ms_hub_url\": \"AI-ModelScope/webqa\",\n      \"columns\": {\n        \"prompt\": \"input\",\n        \"response\": \"output\"\n      }\n    },\n    \"webnovel\": {\n      \"hf_hub_url\": \"zxbsmk/webnovel_cn\",\n      \"ms_hub_url\": \"AI-ModelScope/webnovel_cn\"\n    },\n    \"nectar_sft\": {\n      \"hf_hub_url\": \"AstraMindAI/SFT-Nectar\",\n      \"ms_hub_url\": \"AI-ModelScope/SFT-Nectar\"\n    },\n    \"deepctrl\": {\n      \"ms_hub_url\": \"deepctrl/deepctrl-sft-data\"\n    },\n    \"adgen_train\": {\n      \"hf_hub_url\": \"HasturOfficial/adgen\",\n      \"ms_hub_url\": \"AI-ModelScope/adgen\",\n      \"split\": \"train\",\n      \"columns\": {\n        \"prompt\": \"content\",\n        \"response\": \"summary\"\n      }\n    },\n    \"adgen_eval\": {\n      \"hf_hub_url\": \"HasturOfficial/adgen\",\n      \"ms_hub_url\": \"AI-ModelScope/adgen\",\n      \"split\": \"validation\",\n      \"columns\": {\n        \"prompt\": \"content\",\n        \"response\": \"summary\"\n      }\n    },\n    \"sharegpt_hyper\": {\n      \"hf_hub_url\": \"totally-not-an-llm/sharegpt-hyperfiltered-3k\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"sharegpt4\": {\n      \"hf_hub_url\": \"shibing624/sharegpt_gpt4\",\n      \"ms_hub_url\": \"AI-ModelScope/sharegpt_gpt4\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"ultrachat_200k\": {\n      \"hf_hub_url\": \"HuggingFaceH4/ultrachat_200k\",\n      \"ms_hub_url\": \"AI-ModelScope/ultrachat_200k\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"agent_instruct\": {\n      \"hf_hub_url\": \"THUDM/AgentInstruct\",\n      \"ms_hub_url\": \"ZhipuAI/AgentInstruct\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"lmsys_chat\": {\n      \"hf_hub_url\": \"lmsys/lmsys-chat-1m\",\n      \"ms_hub_url\": \"AI-ModelScope/lmsys-chat-1m\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversation\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"human\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"evol_instruct\": {\n      \"hf_hub_url\": \"WizardLM/WizardLM_evol_instruct_V2_196k\",\n      \"ms_hub_url\": \"AI-ModelScope/WizardLM_evol_instruct_V2_196k\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"glaive_toolcall_100k\": {\n      \"hf_hub_url\": \"hiyouga/glaive-function-calling-v2-sharegpt\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"tools\": \"tools\"\n      }\n    },\n    \"cosmopedia\": {\n      \"hf_hub_url\": \"HuggingFaceTB/cosmopedia\",\n      \"columns\": {\n        \"prompt\": \"prompt\",\n        \"response\": \"text\"\n      }\n    },\n    \"stem_zh\": {\n      \"hf_hub_url\": \"hfl/stem_zh_instruction\"\n    },\n    \"ruozhiba_gpt4\": {\n      \"hf_hub_url\": \"hfl/ruozhiba_gpt4_turbo\"\n    },\n    \"neo_sft\": {\n      \"hf_hub_url\": \"m-a-p/neo_sft_phase2\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"magpie_pro_300k\": {\n      \"hf_hub_url\": \"Magpie-Align/Magpie-Pro-300K-Filtered\",\n      \"formatting\": \"sharegpt\"\n    },\n    \"magpie_ultra\": {\n      \"hf_hub_url\": \"argilla/magpie-ultra-v0.1\",\n      \"columns\": {\n        \"prompt\": \"instruction\",\n        \"response\": \"response\"\n      }\n    },\n    \"web_instruct\": {\n      \"hf_hub_url\": \"TIGER-Lab/WebInstructSub\",\n      \"columns\": {\n        \"prompt\": \"question\",\n        \"response\": \"answer\"\n      }\n    },\n    \"openo1_sft\": {\n      \"hf_hub_url\": \"llamafactory/OpenO1-SFT\",\n      \"ms_hub_url\": \"llamafactory/OpenO1-SFT\",\n      \"columns\": {\n        \"prompt\": \"prompt\",\n        \"response\": \"response\"\n      }\n    },\n    \"open_thoughts\": {\n      \"hf_hub_url\": \"open-thoughts/OpenThoughts-114k\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"system\": \"system\"\n      },\n      \"tags\": {\n        \"role_tag\": \"from\",\n        \"content_tag\": \"value\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"llava_1k_en\": {\n      \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n      \"subset\": \"en\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"images\": \"images\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"llava_1k_zh\": {\n      \"hf_hub_url\": \"BUAADreamer/llava-en-zh-2k\",\n      \"subset\": \"zh\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"images\": \"images\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"llava_150k_en\": {\n      \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n      \"subset\": \"en\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"images\": \"images\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"llava_150k_zh\": {\n      \"hf_hub_url\": \"BUAADreamer/llava-en-zh-300k\",\n      \"subset\": \"zh\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"images\": \"images\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"pokemon_cap\": {\n      \"hf_hub_url\": \"llamafactory/pokemon-gpt4o-captions\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"images\": \"images\"\n      }\n    },\n    \"mllm_pt_demo\": {\n      \"hf_hub_url\": \"BUAADreamer/mllm_pt_demo\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"images\": \"images\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"oasst_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/oasst_de\"\n    },\n    \"dolly_15k_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/dolly-15k_de\"\n    },\n    \"alpaca-gpt4_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/alpaca-gpt4_de\"\n    },\n    \"openschnabeltier_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/openschnabeltier_de\"\n    },\n    \"evol_instruct_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/evol-instruct_de\"\n    },\n    \"dolphin_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/dolphin_de\"\n    },\n    \"booksum_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/booksum_de\"\n    },\n    \"airoboros_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/airoboros-3.0_de\"\n    },\n    \"ultrachat_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/ultra-chat_de\"\n    },\n    \"dpo_en_demo\": {\n      \"file_name\": \"dpo_en_demo.json\",\n      \"ranking\": True,\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\"\n      }\n    },\n    \"dpo_zh_demo\": {\n      \"file_name\": \"dpo_zh_demo.json\",\n      \"ranking\": True,\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\"\n      }\n    },\n    \"dpo_mix_en\": {\n      \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n      \"subset\": \"en\",\n      \"ranking\": True,\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\"\n      }\n    },\n    \"dpo_mix_zh\": {\n      \"hf_hub_url\": \"llamafactory/DPO-En-Zh-20k\",\n      \"subset\": \"zh\",\n      \"ranking\": True,\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\"\n      }\n    },\n    \"ultrafeedback\": {\n      \"hf_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n      \"ms_hub_url\": \"llamafactory/ultrafeedback_binarized\",\n      \"ranking\": True,\n      \"columns\": {\n        \"prompt\": \"instruction\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\"\n      }\n    },\n    \"rlhf_v\": {\n      \"hf_hub_url\": \"llamafactory/RLHF-V\",\n      \"ranking\": True,\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\",\n        \"images\": \"images\"\n      }\n    },\n    \"vlfeedback\": {\n      \"hf_hub_url\": \"Zhihui/VLFeedback\",\n      \"ranking\": True,\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"conversations\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\",\n        \"images\": \"images\"\n      }\n    },\n    \"orca_pairs\": {\n      \"hf_hub_url\": \"Intel/orca_dpo_pairs\",\n      \"ranking\": True,\n      \"columns\": {\n        \"prompt\": \"question\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\",\n        \"system\": \"system\"\n      }\n    },\n    \"hh_rlhf_en\": {\n      \"script_url\": \"hh_rlhf_en\",\n      \"ranking\": True,\n      \"columns\": {\n        \"prompt\": \"instruction\",\n        \"chosen\": \"chosen\",\n        \"rejected\": \"rejected\",\n        \"history\": \"history\"\n      }\n    },\n    \"nectar_rm\": {\n      \"hf_hub_url\": \"AstraMindAI/RLAIF-Nectar\",\n      \"ms_hub_url\": \"AI-ModelScope/RLAIF-Nectar\",\n      \"ranking\": True\n    },\n    \"orca_dpo_de\": {\n      \"hf_hub_url\": \"mayflowergmbh/intel_orca_dpo_pairs_de\",\n      \"ranking\": True\n    },\n    \"kto_en_demo\": {\n      \"file_name\": \"kto_en_demo.json\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"messages\",\n        \"kto_tag\": \"label\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"kto_mix_en\": {\n      \"hf_hub_url\": \"argilla/kto-mix-15k\",\n      \"formatting\": \"sharegpt\",\n      \"columns\": {\n        \"messages\": \"completion\",\n        \"kto_tag\": \"label\"\n      },\n      \"tags\": {\n        \"role_tag\": \"role\",\n        \"content_tag\": \"content\",\n        \"user_tag\": \"user\",\n        \"assistant_tag\": \"assistant\"\n      }\n    },\n    \"ultrafeedback_kto\": {\n      \"hf_hub_url\": \"argilla/ultrafeedback-binarized-preferences-cleaned-kto\",\n      \"ms_hub_url\": \"AI-ModelScope/ultrafeedback-binarized-preferences-cleaned-kto\",\n      \"columns\": {\n        \"prompt\": \"prompt\",\n        \"response\": \"completion\",\n        \"kto_tag\": \"label\"\n      }\n    },\n    \"wiki_demo\": {\n      \"file_name\": \"wiki_demo.txt\",\n      \"columns\": {\n        \"prompt\": \"text\"\n      }\n    },\n    \"c4_demo\": {\n      \"file_name\": \"c4_demo.json\",\n      \"columns\": {\n        \"prompt\": \"text\"\n      }\n    },\n    \"refinedweb\": {\n      \"hf_hub_url\": \"tiiuae/falcon-refinedweb\",\n      \"columns\": {\n        \"prompt\": \"content\"\n      }\n    },\n    \"redpajama_v2\": {\n      \"hf_hub_url\": \"togethercomputer/RedPajama-Data-V2\",\n      \"columns\": {\n        \"prompt\": \"raw_content\"\n      },\n      \"subset\": \"default\"\n    },\n    \"wikipedia_en\": {\n      \"hf_hub_url\": \"olm/olm-wikipedia-20221220\",\n      \"ms_hub_url\": \"AI-ModelScope/olm-wikipedia-20221220\",\n      \"columns\": {\n        \"prompt\": \"text\"\n      }\n    },\n    \"wikipedia_zh\": {\n      \"hf_hub_url\": \"pleisto/wikipedia-cn-20230720-filtered\",\n      \"ms_hub_url\": \"AI-ModelScope/wikipedia-cn-20230720-filtered\",\n      \"columns\": {\n        \"prompt\": \"completion\"\n      }\n    },\n    \"pile\": {\n      \"hf_hub_url\": \"monology/pile-uncopyrighted\",\n      \"ms_hub_url\": \"AI-ModelScope/pile\",\n      \"columns\": {\n        \"prompt\": \"text\"\n      }\n    },\n    \"skypile\": {\n      \"hf_hub_url\": \"Skywork/SkyPile-150B\",\n      \"ms_hub_url\": \"AI-ModelScope/SkyPile-150B\",\n      \"columns\": {\n        \"prompt\": \"text\"\n      }\n    },\n    \"fineweb\": {\n      \"hf_hub_url\": \"HuggingFaceFW/fineweb\",\n      \"columns\": {\n        \"prompt\": \"text\"\n      }\n    },\n    \"fineweb_edu\": {\n      \"hf_hub_url\": \"HuggingFaceFW/fineweb-edu\",\n      \"columns\": {\n        \"prompt\": \"text\"\n      }\n    },\n    \"the_stack\": {\n      \"hf_hub_url\": \"bigcode/the-stack\",\n      \"ms_hub_url\": \"AI-ModelScope/the-stack\",\n      \"columns\": {\n        \"prompt\": \"content\"\n      }\n    },\n    \"darija_finetune_train\": {\n              \"file_name\": \"/kaggle/working/FIneTune/datasets/train.json\",\n              \"columns\": {\n                  \"prompt\": \"instruction\",\n                  \"query\": \"input\",\n                  \"response\": \"output\",\n                  \"system\": \"system\",\n                  \"history\": \"history\"\n              }\n          },\n    \"darija_finetune_val\": {\n              \"file_name\": \"/kaggle/working/FIneTune/datasets/val.json\",\n             \"columns\": {\n                 \"prompt\": \"instruction\",\n                 \"query\": \"input\",\n                  \"response\": \"output\",\n                  \"system\": \"system\",\n                  \"history\": \"history\"\n              }\n    },\n    \"starcoder_python\": {\n      \"hf_hub_url\": \"bigcode/starcoderdata\",\n      \"ms_hub_url\": \"AI-ModelScope/starcoderdata\",\n      \"columns\": {\n        \"prompt\": \"content\"\n      },\n      \"folder\": \"python\"\n    }\n}\n\n# Save to file\nwith open(file_path, \"w\") as file:\n    json.dump(new_content, file, indent=4)\n\nprint(f\"JSON file has been saved to {file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:37:35.053338Z","iopub.execute_input":"2025-03-01T21:37:35.053716Z","iopub.status.idle":"2025-03-01T21:37:35.084676Z","shell.execute_reply.started":"2025-03-01T21:37:35.053680Z","shell.execute_reply":"2025-03-01T21:37:35.083747Z"}},"outputs":[{"name":"stdout","text":"JSON file has been saved to /kaggle/working/LLaMA-Factory/data/dataset_info.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/LLaMA-Factory/examples/train_lora/darija_finetune.yaml\n\n### model\nmodel_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\ntrust_remote_code: true\n\n### method\nstage: sft\ndo_train: true\nfinetuning_type: lora\nlora_rank: 64\nlora_target: all\n\n### dataset\ndataset: darija_finetune_train\neval_dataset: darija_finetune_val\ntemplate: qwen\ncutoff_len: 3500\n# max_samples: 50\noverwrite_cache: true\npreprocessing_num_workers: 16\n\n### output\nresume_from_checkpoint: /kaggle/working/FIneTune/models/checkpoint-15000\noutput_dir: /kaggle/working/FIneTune/models/\nlogging_steps: 10\nsave_steps: 1000\nplot_loss: true\n# overwrite_output_dir: true\n\n### train\nper_device_train_batch_size: 1\ngradient_accumulation_steps: 4\nlearning_rate: 1.0e-4\nnum_train_epochs: 10.0\nlr_scheduler_type: cosine\nwarmup_ratio: 0.1\nbf16: true\nddp_timeout: 180000000\n\n### eval\n# val_size: 0.1\nper_device_eval_batch_size: 1\neval_strategy: steps\neval_steps: 100\n\nreport_to: wandb\nrun_name: last_iter\n\npush_to_hub: true\nexport_hub_model_id: \"ELhadratiOth/darija-english-translater\"\nhub_private_repo: true\nhub_strategy: checkpoint\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:51:18.394627Z","iopub.execute_input":"2025-03-01T21:51:18.395020Z","iopub.status.idle":"2025-03-01T21:51:18.401800Z","shell.execute_reply.started":"2025-03-01T21:51:18.394993Z","shell.execute_reply":"2025-03-01T21:51:18.400800Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/LLaMA-Factory/examples/train_lora/darija_finetune.yaml\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\nos.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"\n!cd LLaMA-Factory/ && llamafactory-cli train /kaggle/working/LLaMA-Factory/examples/train_lora/darija_finetune.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:51:22.740487Z","iopub.execute_input":"2025-03-01T21:51:22.740794Z","iopub.status.idle":"2025-03-01T21:56:52.661030Z","shell.execute_reply.started":"2025-03-01T21:51:22.740767Z","shell.execute_reply":"2025-03-01T21:56:52.660191Z"}},"outputs":[{"name":"stdout","text":"2025-03-01 21:51:28.328978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-01 21:51:28.352687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-01 21:51:28.360085: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[INFO|2025-03-01 21:51:35] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:22329\nW0301 21:51:37.421000 555 torch/distributed/run.py:793] \nW0301 21:51:37.421000 555 torch/distributed/run.py:793] *****************************************\nW0301 21:51:37.421000 555 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0301 21:51:37.421000 555 torch/distributed/run.py:793] *****************************************\n2025-03-01 21:51:43.028029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-01 21:51:43.028348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-01 21:51:43.051254: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-01 21:51:43.051774: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-01 21:51:43.058549: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-01 21:51:43.059246: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[WARNING|2025-03-01 21:51:48] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n[INFO|2025-03-01 21:51:48] llamafactory.hparams.parser:384 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n[INFO|2025-03-01 21:51:48] llamafactory.hparams.parser:384 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\n[INFO|configuration_utils.py:699] 2025-03-01 21:51:48,207 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n[INFO|configuration_utils.py:771] 2025-03-01 21:51:48,209 >> Model config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:48,281 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:48,281 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:48,281 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:48,281 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:48,281 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:48,281 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:48,281 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2313] 2025-03-01 21:51:48,693 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|configuration_utils.py:699] 2025-03-01 21:51:48,981 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n[INFO|configuration_utils.py:771] 2025-03-01 21:51:48,982 >> Model config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:49,055 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:49,055 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:49,056 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:49,056 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:49,056 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:49,056 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2050] 2025-03-01 21:51:49,056 >> loading file chat_template.jinja from cache at None\n[INFO|tokenization_utils_base.py:2313] 2025-03-01 21:51:49,417 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n[INFO|2025-03-01 21:51:49] llamafactory.data.template:157 >> Add <|im_end|> to stop words.\n[INFO|2025-03-01 21:51:49] llamafactory.data.loader:157 >> Loading dataset /kaggle/working/FIneTune/datasets/train.json...\nConverting format of dataset (num_proc=16):   0%| | 0/12000 [00:00<?, ? examples[rank1]:[W301 21:51:49.335716733 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\nConverting format of dataset (num_proc=16): 100%|| 12000/12000 [00:01<00:00, 10\n[INFO|2025-03-01 21:51:51] llamafactory.data.loader:157 >> Loading dataset /kaggle/working/FIneTune/datasets/val.json...\nConverting format of dataset (num_proc=16): 100%|| 1000/1000 [00:00<00:00, 2408\n[rank0]:[W301 21:51:52.483796065 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\nRunning tokenizer on dataset (num_proc=16): 100%|| 12000/12000 [00:11<00:00, 10\ntraining example:\ninput_ids:\n[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 264, 923, 624, 5404, 537, 6923, 894, 4960, 1467, 26, 2677, 387, 12966, 304, 697, 14468, 13, 151645, 198, 151644, 872, 198, 2, 422, 2780, 5580, 5571, 510, 144225, 145312, 32495, 119, 241, 144368, 144896, 32495, 118, 115, 144651, 145148, 32495, 118, 119, 144651, 144506, 144123, 32495, 119, 104, 145103, 144289, 144697, 32495, 119, 233, 144292, 144055, 145273, 32495, 118, 239, 144292, 144445, 32495, 119, 249, 140672, 144383, 144583, 32495, 119, 249, 144289, 144225, 145204, 32495, 118, 239, 140672, 133127, 144566, 144847, 32495, 118, 102, 144383, 140672, 139494, 145273, 32495, 118, 245, 144512, 144289, 144383, 144332, 140672, 32495, 119, 241, 144055, 144866, 32495, 119, 96, 144292, 144055, 144309, 144614, 32495, 118, 235, 144524, 146012, 32495, 118, 102, 142673, 145566, 32495, 118, 235, 139494, 144542, 144289, 144225, 145204, 32495, 118, 102, 144397, 144309, 144895, 32495, 119, 96, 144728, 144292, 144851, 144123, 32495, 119, 241, 144055, 144866, 32495, 119, 255, 142673, 139494, 144651, 144199, 144301, 144123, 32495, 119, 100, 140672, 144383, 145103, 144123, 198, 2, 5430, 510, 27473, 279, 422, 2780, 5580, 1467, 1119, 6364, 624, 2, 31021, 9258, 15042, 510, 4913, 13193, 788, 5212, 3053, 788, 5212, 4684, 788, 330, 22574, 14468, 315, 264, 422, 2780, 5580, 11652, 497, 330, 2102, 788, 330, 24412, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 3053, 7914, 330, 2102, 788, 330, 24412, 497, 330, 1313, 788, 330, 1700, 16707, 2, 6364, 38041, 549, 151645, 198, 151644, 77091, 198, 9454, 11, 358, 3520, 5485, 264, 2150, 429, 14897, 911, 498, 11, 5488, 429, 1052, 374, 264, 1874, 448, 697, 829, 429, 702, 4558, 264, 3526, 1251, 13, 2938, 1874, 374, 264, 9519, 28688, 323, 1052, 374, 10923, 680, 16062, 304, 432, 13, 151645, 198]\ninputs:\n<|im_start|>system\nYou are a professional NLP data parser.\nFollow the provided `Task` by the user and the `Output Scheme` to generate a String.\nDo not generate any extra text; always be consistent in your translation.<|im_end|>\n<|im_start|>user\n# Darija Input:\n                     \n# Task:\nTranslate the Darija text into English.\n# Expected Output Format:\n{\"properties\": {\"translation\": {\"description\": \"English translation of a Darija sentence\", \"title\": \"Translation\", \"type\": \"string\"}}, \"required\": [\"translation\"], \"title\": \"Translation\", \"type\": \"object\"}\n# English Translation :<|im_end|>\n<|im_start|>assistant\nYes, I actually saw a page that talked about you, saying that there is a group with your name that has almost a million people. That group is a mixed invitation and there is sedition rising in it.<|im_end|>\n\nlabel_ids:\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9454, 11, 358, 3520, 5485, 264, 2150, 429, 14897, 911, 498, 11, 5488, 429, 1052, 374, 264, 1874, 448, 697, 829, 429, 702, 4558, 264, 3526, 1251, 13, 2938, 1874, 374, 264, 9519, 28688, 323, 1052, 374, 10923, 680, 16062, 304, 432, 13, 151645, 198]\nlabels:\nYes, I actually saw a page that talked about you, saying that there is a group with your name that has almost a million people. That group is a mixed invitation and there is sedition rising in it.<|im_end|>\n\nRunning tokenizer on dataset (num_proc=16): 100%|| 1000/1000 [00:07<00:00, 134.\neval example:\ninput_ids:\n[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 264, 923, 624, 5404, 537, 6923, 894, 4960, 1467, 26, 2677, 387, 12966, 304, 697, 14468, 13, 151645, 198, 151644, 872, 198, 2, 422, 2780, 5580, 5571, 510, 144430, 140672, 144352, 145566, 32495, 115, 110, 32495, 119, 241, 144055, 145273, 32495, 118, 235, 144945, 144445, 32495, 118, 96, 144292, 144055, 144122, 144123, 198, 2, 5430, 510, 27473, 279, 422, 2780, 5580, 1467, 1119, 6364, 624, 2, 31021, 9258, 15042, 510, 4913, 13193, 788, 5212, 3053, 788, 5212, 4684, 788, 330, 22574, 14468, 315, 264, 422, 2780, 5580, 11652, 497, 330, 2102, 788, 330, 24412, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 3053, 7914, 330, 2102, 788, 330, 24412, 497, 330, 1313, 788, 330, 1700, 16707, 2, 6364, 38041, 549, 151645, 198, 151644, 77091, 198, 11109, 4264, 16072, 498, 11, 10641, 19851, 7523, 13, 151645, 198]\ninputs:\n<|im_start|>system\nYou are a professional NLP data parser.\nFollow the provided `Task` by the user and the `Output Scheme` to generate a String.\nDo not generate any extra text; always be consistent in your translation.<|im_end|>\n<|im_start|>user\n# Darija Input:\n    \n# Task:\nTranslate the Darija text into English.\n# Expected Output Format:\n{\"properties\": {\"translation\": {\"description\": \"English translation of a Darija sentence\", \"title\": \"Translation\", \"type\": \"string\"}}, \"required\": [\"translation\"], \"title\": \"Translation\", \"type\": \"object\"}\n# English Translation :<|im_end|>\n<|im_start|>assistant\nMay God bless you, brother Halima.<|im_end|>\n\nlabel_ids:\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 11109, 4264, 16072, 498, 11, 10641, 19851, 7523, 13, 151645, 198]\nlabels:\nMay God bless you, brother Halima.<|im_end|>\n\n[INFO|configuration_utils.py:699] 2025-03-01 21:52:13,224 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n[INFO|configuration_utils.py:771] 2025-03-01 21:52:13,225 >> Model config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n[INFO|modeling_utils.py:3982] 2025-03-01 21:52:13,289 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors\n[INFO|modeling_utils.py:1633] 2025-03-01 21:52:13,303 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n[INFO|configuration_utils.py:1140] 2025-03-01 21:52:13,306 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\n[WARNING|logging.py:329] 2025-03-01 21:52:13,308 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\nSliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n[INFO|modeling_utils.py:4970] 2025-03-01 21:52:14,806 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\n[INFO|modeling_utils.py:4978] 2025-03-01 21:52:14,806 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n[INFO|configuration_utils.py:1095] 2025-03-01 21:52:14,954 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json\n[INFO|configuration_utils.py:1140] 2025-03-01 21:52:14,954 >> Generate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\n[INFO|2025-03-01 21:52:14] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n[INFO|2025-03-01 21:52:14] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n[INFO|2025-03-01 21:52:14] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n[INFO|2025-03-01 21:52:14] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n[INFO|2025-03-01 21:52:14] llamafactory.model.model_utils.misc:157 >> Found linear modules: up_proj,down_proj,q_proj,o_proj,gate_proj,v_proj,k_proj\n[INFO|2025-03-01 21:52:15] llamafactory.model.loader:157 >> trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n[INFO|trainer.py:746] 2025-03-01 21:52:17,714 >> Using auto half precision backend\n[WARNING|trainer.py:781] 2025-03-01 21:52:17,716 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n[INFO|trainer.py:2789] 2025-03-01 21:52:17,717 >> Loading model from /kaggle/working/FIneTune/models/checkpoint-15000.\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n[INFO|trainer.py:2405] 2025-03-01 21:52:19,450 >> ***** Running training *****\n[INFO|trainer.py:2406] 2025-03-01 21:52:19,450 >>   Num examples = 12,000\n[INFO|trainer.py:2407] 2025-03-01 21:52:19,450 >>   Num Epochs = 10\n[INFO|trainer.py:2408] 2025-03-01 21:52:19,450 >>   Instantaneous batch size per device = 1\n[INFO|trainer.py:2411] 2025-03-01 21:52:19,450 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n[INFO|trainer.py:2412] 2025-03-01 21:52:19,450 >>   Gradient Accumulation steps = 4\n[INFO|trainer.py:2413] 2025-03-01 21:52:19,450 >>   Total optimization steps = 15,000\n[INFO|trainer.py:2414] 2025-03-01 21:52:19,455 >>   Number of trainable parameters = 73,859,072\n[INFO|trainer.py:2436] 2025-03-01 21:52:19,460 >>   Continuing training from checkpoint, will skip to saved global_step\n[INFO|trainer.py:2437] 2025-03-01 21:52:19,460 >>   Continuing training from epoch 10\n[INFO|trainer.py:2438] 2025-03-01 21:52:19,460 >>   Continuing training from global step 15000\n[INFO|trainer.py:2440] 2025-03-01 21:52:19,460 >>   Will skip the first 10 epochs then the first 0 batches in the first epoch.\n[INFO|integration_utils.py:817] 2025-03-01 21:52:19,465 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mothmanelhadrati\u001b[0m (\u001b[33mothmanelhadrati-ensa-al-hoceima\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/LLaMA-Factory/wandb/run-20250301_215219-krudhm3g\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlast_iter\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/othmanelhadrati-ensa-al-hoceima/llamafactory\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/othmanelhadrati-ensa-al-hoceima/llamafactory/runs/krudhm3g\u001b[0m\n  0%|                                                 | 0/15000 [00:00<?, ?it/s][INFO|trainer.py:2657] 2025-03-01 21:52:20,588 >> \n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n{'train_runtime': 1.1327, 'train_samples_per_second': 105944.302, 'train_steps_per_second': 13243.038, 'train_loss': 0.0, 'epoch': 10.0}\n  0%|                                                 | 0/15000 [00:00<?, ?it/s]\n[INFO|trainer.py:3942] 2025-03-01 21:52:20,605 >> Saving model checkpoint to /kaggle/working/FIneTune/models/\n[INFO|configuration_utils.py:699] 2025-03-01 21:52:20,797 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n[INFO|configuration_utils.py:771] 2025-03-01 21:52:20,798 >> Model config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n[INFO|tokenization_utils_base.py:2500] 2025-03-01 21:52:21,639 >> tokenizer config file saved in /kaggle/working/FIneTune/models/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2509] 2025-03-01 21:52:21,640 >> Special tokens file saved in /kaggle/working/FIneTune/models/special_tokens_map.json\n[INFO|trainer.py:3942] 2025-03-01 21:52:21,865 >> Saving model checkpoint to /kaggle/working/FIneTune/models/\n[INFO|configuration_utils.py:699] 2025-03-01 21:52:22,046 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n[INFO|configuration_utils.py:771] 2025-03-01 21:52:22,047 >> Model config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n[INFO|tokenization_utils_base.py:2500] 2025-03-01 21:52:23,110 >> tokenizer config file saved in /kaggle/working/FIneTune/models/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2509] 2025-03-01 21:52:23,111 >> Special tokens file saved in /kaggle/working/FIneTune/models/special_tokens_map.json\nadapter_model.safetensors:   0%|                     | 0.00/295M [00:00<?, ?B/s]\n\nUpload 3 LFS files:   0%|                                 | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\ntokenizer.json:   0%|                               | 0.00/11.4M [00:00<?, ?B/s]\u001b[A\n\n\nadapter_model.safetensors:   0%|            | 1.10M/295M [00:00<00:26, 11.0MB/s]\u001b[A\u001b[A\u001b[A\ntraining_args.bin: 100%|| 5.75k/5.75k [00:00<00:00, 56.7kB/s]\u001b[A\ntokenizer.json: 100%|| 11.4M/11.4M [00:00<00:00, 38.4MB/s]\nadapter_model.safetensors: 100%|| 295M/295M [00:05<00:00, 50.9MB/s]\n\n\nUpload 3 LFS files: 100%|| 3/3 [00:06<00:00,  2.00s/it]\u001b[A\u001b[A\n***** train metrics *****\n  epoch                    =        10.0\n  total_flos               = 214408548GF\n  train_loss               =         0.0\n  train_runtime            =  0:00:01.13\n  train_samples_per_second =  105944.302\n  train_steps_per_second   =   13243.038\nFigure saved at: /kaggle/working/FIneTune/models/training_loss.png\n[WARNING|2025-03-01 21:52:36] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n[WARNING|2025-03-01 21:52:36] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n[INFO|trainer.py:4258] 2025-03-01 21:52:36,435 >> \n***** Running Evaluation *****\n[INFO|trainer.py:4260] 2025-03-01 21:52:36,435 >>   Num examples = 1000\n[INFO|trainer.py:4263] 2025-03-01 21:52:36,436 >>   Batch size = 1\n100%|| 500/500 [04:06<00:00,  2.03it/s]\n***** eval metrics *****\n  epoch                                       =       10.0\n  eval_darija_finetune_val_loss               =      1.674\n  eval_darija_finetune_val_runtime            = 0:04:07.60\n  eval_darija_finetune_val_samples_per_second =      4.039\n  eval_darija_finetune_val_steps_per_second   =      2.019\n[INFO|trainer.py:3942] 2025-03-01 21:56:44,046 >> Saving model checkpoint to /kaggle/working/FIneTune/models/\n[INFO|configuration_utils.py:699] 2025-03-01 21:56:44,281 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n[INFO|configuration_utils.py:771] 2025-03-01 21:56:44,282 >> Model config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": 32768,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.49.0\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n[INFO|tokenization_utils_base.py:2500] 2025-03-01 21:56:45,144 >> tokenizer config file saved in /kaggle/working/FIneTune/models/tokenizer_config.json\n[INFO|tokenization_utils_base.py:2509] 2025-03-01 21:56:45,145 >> Special tokens file saved in /kaggle/working/FIneTune/models/special_tokens_map.json\n[INFO|modelcard.py:449] 2025-03-01 21:56:45,427 >> Dropping the following result as it does not have all the necessary fields:\n{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n[rank0]:[W301 21:56:48.829707504 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport  pandas as pd\nimport os\nfrom os.path import join\nfrom pydantic import BaseModel, Field\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\n\ndata_dir = \"/kaggle/working/FIneTune/\"\nbase_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\ndevice = \"cuda\"\ntorch_dtype = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:15:44.690430Z","iopub.execute_input":"2025-03-01T21:15:44.690742Z","iopub.status.idle":"2025-03-01T21:15:44.695725Z","shell.execute_reply.started":"2025-03-01T21:15:44.690715Z","shell.execute_reply":"2025-03-01T21:15:44.694879Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    device_map=\"auto\",\n    torch_dtype = torch_dtype\n)\n\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T20:49:34.770678Z","iopub.execute_input":"2025-03-01T20:49:34.771182Z","iopub.status.idle":"2025-03-01T20:50:15.827881Z","shell.execute_reply.started":"2025-03-01T20:49:34.771153Z","shell.execute_reply":"2025-03-01T20:50:15.826954Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00e2651657174870a245db3b040decb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa7d3ab0d8b04d52bae5bc6b45ba34a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc69597f932c41adbc4c3aebd72ac461"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d2ff6f790a849d28fc3b78166db01ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2589b2f623f42cc85d95f120c947a55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1812fb7c5fe4fcd97e9d2ca0327ebdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01bce19ddc1d4b2d84beac8b6f5824f8"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"finetuned_model_id = \"/kaggle/working/FIneTune/models\"\nmodel.load_adapter(finetuned_model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T20:50:22.572118Z","iopub.execute_input":"2025-03-01T20:50:22.572713Z","iopub.status.idle":"2025-03-01T20:50:24.250940Z","shell.execute_reply.started":"2025-03-01T20:50:22.572684Z","shell.execute_reply":"2025-03-01T20:50:24.250244Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from pydantic import BaseModel, Field\nclass Translation(BaseModel):\n  translation: str = Field(... , description=\"translation of the text\")\n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T20:50:25.894012Z","iopub.execute_input":"2025-03-01T20:50:25.894330Z","iopub.status.idle":"2025-03-01T20:50:25.934475Z","shell.execute_reply.started":"2025-03-01T20:50:25.894301Z","shell.execute_reply":"2025-03-01T20:50:25.933855Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import  json\ndef generate_resp(messages):\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    print(text)\n\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=1024,\n        do_sample=False, top_k=None, temperature=0.8, top_p=None,\n    )\n\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return response\n\n\n    \nmessage = [\n    {\n        \"role\": \"system\",\n        \"content\": \"\\n\".join([\n         \"You are a professional NLP data parser.\",\n    \"Follow the provided `Task` by the user and the `Output Scheme` to generate a String.\",\n    \"Do not generate any extra text; always be consistent in your translation.\"\n        ])\n    },\n    {\n        \"role\": \"user\",\n        \"content\":  \"\\n\".join([\n            \"## Task:\",\n            \"llah inej7ek\".strip(),\n            \"\",\n\n            \"## Pydantic Details:\",\n           json.dumps(Translation.model_json_schema(), ensure_ascii=False),\n            \"\",\n\n            \"## # English Translation :\",\n            \n\n        ])\n    }\n]\n\nresponse = generate_resp(message)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:06:39.726605Z","iopub.execute_input":"2025-03-01T21:06:39.726907Z","iopub.status.idle":"2025-03-01T21:06:40.261668Z","shell.execute_reply.started":"2025-03-01T21:06:39.726881Z","shell.execute_reply":"2025-03-01T21:06:40.261050Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a professional NLP data parser.\nFollow the provided `Task` by the user and the `Output Scheme` to generate a String.\nDo not generate any extra text; always be consistent in your translation.<|im_end|>\n<|im_start|>user\n## Task:\nllah inej7ek\n\n## Pydantic Details:\n{\"properties\": {\"translation\": {\"description\": \"translation of the text\", \"title\": \"Translation\", \"type\": \"string\"}}, \"required\": [\"translation\"], \"title\": \"Translation\", \"type\": \"object\"}\n\n## # English Translation :<|im_end|>\n<|im_start|>assistant\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:06:40.623961Z","iopub.execute_input":"2025-03-01T21:06:40.624272Z","iopub.status.idle":"2025-03-01T21:06:40.629195Z","shell.execute_reply.started":"2025-03-01T21:06:40.624249Z","shell.execute_reply":"2025-03-01T21:06:40.628512Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'May God bless you.'"},"metadata":{}}],"execution_count":25}]}